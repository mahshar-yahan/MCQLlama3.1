{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-29T11:57:30.087484Z",
     "iopub.status.busy": "2024-10-29T11:57:30.086976Z",
     "iopub.status.idle": "2024-10-29T11:57:30.453362Z",
     "shell.execute_reply": "2024-10-29T11:57:30.452459Z",
     "shell.execute_reply.started": "2024-10-29T11:57:30.087445Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/model.safetensors.index.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00003-of-00004.safetensors\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/config.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00001-of-00004.safetensors\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/README.md\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/USE_POLICY.md\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/tokenizer.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/tokenizer_config.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00004-of-00004.safetensors\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/special_tokens_map.json\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00002-of-00004.safetensors\n",
      "/kaggle/input/llama-3.1/transformers/8b-instruct/1/generation_config.json\n",
      "/kaggle/input/banglamcq-dataset/BanglaMCQ-Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T11:57:35.243385Z",
     "iopub.status.busy": "2024-10-29T11:57:35.242083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U bitsandbytes\n",
    "%pip install -U transformers\n",
    "%pip install -U accelerate\n",
    "%pip install -U peft\n",
    "%pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:55:12.220299Z",
     "iopub.status.busy": "2024-09-21T14:55:12.219504Z",
     "iopub.status.idle": "2024-09-21T14:55:12.358982Z",
     "shell.execute_reply": "2024-09-21T14:55:12.357988Z",
     "shell.execute_reply.started": "2024-09-21T14:55:12.220252Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>ans_txt</th>\n",
       "      <th>answer0</th>\n",
       "      <th>answer1</th>\n",
       "      <th>answer2</th>\n",
       "      <th>answer3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>বাংলাদেশ একটি ক্ষুদ্র আয়তনের জনবহুল দেশ এ দেশে...</td>\n",
       "      <td>বাংলাদেশের আয়তন কত বর্গকিলোমিটার</td>\n",
       "      <td>১৪৭৫৭০</td>\n",
       "      <td>১৪০৫৭০</td>\n",
       "      <td>১৪৭৫৭০</td>\n",
       "      <td>৪৭৫৭০</td>\n",
       "      <td>৫৬৭৫৭০</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>বাংলাদেশ একটি ক্ষুদ্র আয়তনের জনবহুল দেশ এ দেশে...</td>\n",
       "      <td>মোট আয়তনের কত শতাংশ বনভূমি</td>\n",
       "      <td>১৭</td>\n",
       "      <td>২০</td>\n",
       "      <td>১৭</td>\n",
       "      <td>৫৬</td>\n",
       "      <td>৩০</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>বাংলাদেশ একটি ক্ষুদ্র আয়তনের জনবহুল দেশ এ দেশে...</td>\n",
       "      <td>বাংলাদেশের রাজধানীর নাম কি</td>\n",
       "      <td>ঢাকা</td>\n",
       "      <td>ঢাকা</td>\n",
       "      <td>চট্টগ্রাম</td>\n",
       "      <td>সিলেট</td>\n",
       "      <td>বরিশাল</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>বাংলাদেশ একটি ক্ষুদ্র আয়তনের জনবহুল দেশ এ দেশে...</td>\n",
       "      <td>বাংলাদেশে মোট কয়টি বিভাগীয় শহর রয়েছে</td>\n",
       "      <td>আটটি</td>\n",
       "      <td>দশটি</td>\n",
       "      <td>বিশটি</td>\n",
       "      <td>আটটি</td>\n",
       "      <td>এগারোটি</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>বাংলাদেশের মোট জনসংখ্যা প্রায় ১৬ কোটির মতো পৃথ...</td>\n",
       "      <td>বাংলাদেশের মোট জনসংখ্যা কয় কোটি</td>\n",
       "      <td>১৬</td>\n",
       "      <td>১৫</td>\n",
       "      <td>১৮</td>\n",
       "      <td>৫৬</td>\n",
       "      <td>১৬</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  বাংলাদেশ একটি ক্ষুদ্র আয়তনের জনবহুল দেশ এ দেশে...   \n",
       "1  বাংলাদেশ একটি ক্ষুদ্র আয়তনের জনবহুল দেশ এ দেশে...   \n",
       "2  বাংলাদেশ একটি ক্ষুদ্র আয়তনের জনবহুল দেশ এ দেশে...   \n",
       "3  বাংলাদেশ একটি ক্ষুদ্র আয়তনের জনবহুল দেশ এ দেশে...   \n",
       "4  বাংলাদেশের মোট জনসংখ্যা প্রায় ১৬ কোটির মতো পৃথ...   \n",
       "\n",
       "                               question ans_txt answer0     answer1 answer2  \\\n",
       "0      বাংলাদেশের আয়তন কত বর্গকিলোমিটার  ১৪৭৫৭০  ১৪০৫৭০      ১৪৭৫৭০   ৪৭৫৭০   \n",
       "1            মোট আয়তনের কত শতাংশ বনভূমি      ১৭      ২০          ১৭      ৫৬   \n",
       "2            বাংলাদেশের রাজধানীর নাম কি    ঢাকা    ঢাকা  চট্টগ্রাম   সিলেট    \n",
       "3  বাংলাদেশে মোট কয়টি বিভাগীয় শহর রয়েছে    আটটি   দশটি        বিশটি    আটটি   \n",
       "4       বাংলাদেশের মোট জনসংখ্যা কয় কোটি      ১৬      ১৫          ১৮      ৫৬   \n",
       "\n",
       "   answer3  label  \n",
       "0   ৫৬৭৫৭০    1.0  \n",
       "1       ৩০    1.0  \n",
       "2   বরিশাল    0.0  \n",
       "3  এগারোটি    2.0  \n",
       "4       ১৬    3.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/banglamcq-dataset/BanglaMCQ-Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:55:12.361895Z",
     "iopub.status.busy": "2024-09-21T14:55:12.361562Z",
     "iopub.status.idle": "2024-09-21T14:55:12.378136Z",
     "shell.execute_reply": "2024-09-21T14:55:12.377343Z",
     "shell.execute_reply.started": "2024-09-21T14:55:12.361860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame and select only 3000 rows (adjust as needed)\n",
    "df = df.sample(frac=1, random_state=85).reset_index(drop=True)[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:55:12.379629Z",
     "iopub.status.busy": "2024-09-21T14:55:12.379318Z",
     "iopub.status.idle": "2024-09-21T14:55:12.397909Z",
     "shell.execute_reply": "2024-09-21T14:55:12.396989Z",
     "shell.execute_reply.started": "2024-09-21T14:55:12.379595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Handle unexpected label values\n",
    "def clean_label(label):\n",
    "    if pd.isna(label) or label not in [0, 1, 2, 3]:\n",
    "        return np.nan\n",
    "    return int(label)\n",
    "\n",
    "df['label'] = df['label'].apply(clean_label)\n",
    "df = df.dropna(subset=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:55:12.399430Z",
     "iopub.status.busy": "2024-09-21T14:55:12.399098Z",
     "iopub.status.idle": "2024-09-21T14:55:12.409073Z",
     "shell.execute_reply": "2024-09-21T14:55:12.408205Z",
     "shell.execute_reply.started": "2024-09-21T14:55:12.399397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['label'] = df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:55:12.410614Z",
     "iopub.status.busy": "2024-09-21T14:55:12.410247Z",
     "iopub.status.idle": "2024-09-21T14:55:12.419712Z",
     "shell.execute_reply": "2024-09-21T14:55:12.418802Z",
     "shell.execute_reply.started": "2024-09-21T14:55:12.410582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split the DataFrame\n",
    "train_size = 0.8\n",
    "eval_size = 0.1\n",
    "train_end = int(train_size * len(df))\n",
    "eval_end = train_end + int(eval_size * len(df))\n",
    "X_train = df[:train_end]\n",
    "X_eval = df[train_end:eval_end]\n",
    "X_test = df[eval_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:55:12.421383Z",
     "iopub.status.busy": "2024-09-21T14:55:12.421048Z",
     "iopub.status.idle": "2024-09-21T14:55:12.431782Z",
     "shell.execute_reply": "2024-09-21T14:55:12.430966Z",
     "shell.execute_reply.started": "2024-09-21T14:55:12.421351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define prompt generation functions\n",
    "def generate_prompt(data_point):\n",
    "    pass # needed to create prompt funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:55:12.433434Z",
     "iopub.status.busy": "2024-09-21T14:55:12.433114Z",
     "iopub.status.idle": "2024-09-21T14:55:12.552124Z",
     "shell.execute_reply": "2024-09-21T14:55:12.551222Z",
     "shell.execute_reply.started": "2024-09-21T14:55:12.433403Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/3965882420.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\n",
      "/tmp/ipykernel_36/3965882420.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Generate prompts for training and evaluation data\n",
    "X_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\n",
    "X_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)\n",
    "\n",
    "# Convert to datasets\n",
    "train_data = Dataset.from_pandas(X_train[[\"text\"]])\n",
    "eval_data = Dataset.from_pandas(X_eval[[\"text\"]])\n",
    "\n",
    "# Generate test prompts and extract true labels\n",
    "y_true = X_test.loc[:,'label']\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:56:31.436326Z",
     "iopub.status.busy": "2024-09-21T14:56:31.436002Z",
     "iopub.status.idle": "2024-09-21T14:56:31.443452Z",
     "shell.execute_reply": "2024-09-21T14:56:31.442545Z",
     "shell.execute_reply.started": "2024-09-21T14:56:31.436286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(test, model, tokenizer):\n",
    "    # y_pred = []\n",
    "    # options = ['ক', 'খ', 'গ', 'ঘ']\n",
    "    \n",
    "    # for i in tqdm(range(len(test))):\n",
    "    #     prompt = test.iloc[i][\"text\"]\n",
    "    #     pipe = pipeline(task=\"text-generation\", \n",
    "    #                     model=model, \n",
    "    #                     tokenizer=tokenizer, \n",
    "    #                     max_new_tokens=2, \n",
    "    #                     temperature=0.1)\n",
    "        \n",
    "    #     result = pipe(prompt)\n",
    "    #     answer = result[0]['generated_text'].split(\"সঠিক উত্তর:\")[-1].strip()\n",
    "        \n",
    "    #     for idx, option in enumerate(options):\n",
    "    #         if option in answer:\n",
    "    #             y_pred.append(idx)\n",
    "    #             break\n",
    "    #     else:\n",
    "    #         y_pred.append(-1)  # Invalid prediction\n",
    "    \n",
    "    # return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:56:31.444979Z",
     "iopub.status.busy": "2024-09-21T14:56:31.444702Z",
     "iopub.status.idle": "2024-09-21T14:56:31.455990Z",
     "shell.execute_reply": "2024-09-21T14:56:31.455205Z",
     "shell.execute_reply.started": "2024-09-21T14:56:31.444949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(y_true, y_pred):\n",
    "    labels = [0, 1, 2, 3]\n",
    "    \n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    for label in labels:\n",
    "        label_indices = [i for i in range(len(y_true)) if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for option {[\"ক\", \"খ\", \"গ\", \"ঘ\"][label]}: {label_accuracy:.3f}')\n",
    "        \n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred, target_names=[\"ক\", \"খ\", \"গ\", \"ঘ\"], labels=labels)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=labels)\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:55:12.555133Z",
     "iopub.status.busy": "2024-09-21T14:55:12.554823Z",
     "iopub.status.idle": "2024-09-21T14:56:31.434915Z",
     "shell.execute_reply": "2024-09-21T14:56:31.434038Z",
     "shell.execute_reply.started": "2024-09-21T14:55:12.555099Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03111fea8c384ca4b99a4c0aa7d66eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model setup\n",
    "base_model_name = \"/kaggle/input/llama-3.1/transformers/8b-instruct/1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:31:56.285385Z",
     "iopub.status.busy": "2024-09-21T14:31:56.285025Z",
     "iopub.status.idle": "2024-09-21T14:36:32.811309Z",
     "shell.execute_reply": "2024-09-21T14:36:32.810393Z",
     "shell.execute_reply.started": "2024-09-21T14:31:56.285344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [04:36<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# y_pred_org = predict(X_test, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:37:01.814822Z",
     "iopub.status.busy": "2024-09-21T14:37:01.814079Z",
     "iopub.status.idle": "2024-09-21T14:37:01.819001Z",
     "shell.execute_reply": "2024-09-21T14:37:01.818058Z",
     "shell.execute_reply.started": "2024-09-21T14:37:01.814781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# y_ture_list = y_true.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:37:18.459265Z",
     "iopub.status.busy": "2024-09-21T14:37:18.458857Z",
     "iopub.status.idle": "2024-09-21T14:37:18.479049Z",
     "shell.execute_reply": "2024-09-21T14:37:18.478054Z",
     "shell.execute_reply.started": "2024-09-21T14:37:18.459226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.799\n",
      "Accuracy for option ক: 0.684\n",
      "Accuracy for option খ: 0.882\n",
      "Accuracy for option গ: 0.923\n",
      "Accuracy for option ঘ: 0.711\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           ক       0.96      0.68      0.80        38\n",
      "           খ       0.59      0.88      0.71        34\n",
      "           গ       0.84      0.92      0.88        39\n",
      "           ঘ       0.96      0.71      0.82        38\n",
      "\n",
      "    accuracy                           0.80       149\n",
      "   macro avg       0.84      0.80      0.80       149\n",
      "weighted avg       0.84      0.80      0.80       149\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[26 11  1  0]\n",
      " [ 0 30  3  1]\n",
      " [ 0  3 36  0]\n",
      " [ 1  7  3 27]]\n"
     ]
    }
   ],
   "source": [
    "# Compare with the original model\n",
    "print(\"\\nOriginal Model Evaluation:\")\n",
    "evaluate(y_ture_list, y_pred_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:56:31.457607Z",
     "iopub.status.busy": "2024-09-21T14:56:31.457204Z",
     "iopub.status.idle": "2024-09-21T14:56:31.471788Z",
     "shell.execute_reply": "2024-09-21T14:56:31.470926Z",
     "shell.execute_reply.started": "2024-09-21T14:56:31.457568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Find all linear module names\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:56:31.473557Z",
     "iopub.status.busy": "2024-09-21T14:56:31.473114Z",
     "iopub.status.idle": "2024-09-21T14:56:33.343168Z",
     "shell.execute_reply": "2024-09-21T14:56:33.342339Z",
     "shell.execute_reply.started": "2024-09-21T14:56:31.473504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd484719bfcb4b9caebc351506b59372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce594e2fa3e6400e8b5d98ae3421dce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/148 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "\n",
    "# Set up training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,\n",
    "    logging_steps=500,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T14:56:33.344512Z",
     "iopub.status.busy": "2024-09-21T14:56:33.344212Z",
     "iopub.status.idle": "2024-09-21T18:00:05.907433Z",
     "shell.execute_reply": "2024-09-21T18:00:05.906473Z",
     "shell.execute_reply.started": "2024-09-21T14:56:33.344479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1782' max='1782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1782/1782 3:03:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.061600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.045500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1782, training_loss=0.1023531035006916, metrics={'train_runtime': 11011.1385, 'train_samples_per_second': 0.324, 'train_steps_per_second': 0.162, 'total_flos': 8.132727747966566e+16, 'train_loss': 0.1023531035006916, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T18:02:02.431196Z",
     "iopub.status.busy": "2024-09-21T18:02:02.430408Z",
     "iopub.status.idle": "2024-09-21T18:02:02.585470Z",
     "shell.execute_reply": "2024-09-21T18:02:02.584653Z",
     "shell.execute_reply.started": "2024-09-21T18:02:02.431129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "trainer.model.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T18:03:57.560617Z",
     "iopub.status.busy": "2024-09-21T18:03:57.559813Z",
     "iopub.status.idle": "2024-09-21T18:03:58.058006Z",
     "shell.execute_reply": "2024-09-21T18:03:58.056685Z",
     "shell.execute_reply.started": "2024-09-21T18:03:57.560578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the fine-tuned model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fine_tuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/fine_tuned_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3909\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3906\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   3908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3909\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3911\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3912\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:86\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     83\u001b[0m         key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\n\u001b[1;32m     84\u001b[0m     }\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/kaggle/working/fine_tuned_model\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T18:07:04.979385Z",
     "iopub.status.busy": "2024-09-21T18:07:04.978498Z",
     "iopub.status.idle": "2024-09-21T18:11:59.289440Z",
     "shell.execute_reply": "2024-09-21T18:11:59.288505Z",
     "shell.execute_reply.started": "2024-09-21T18:07:04.979344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [04:54<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the fine-tuned model\n",
    "y_pred_fine_tuned = predict(X_test, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T18:13:37.508812Z",
     "iopub.status.busy": "2024-09-21T18:13:37.508434Z",
     "iopub.status.idle": "2024-09-21T18:13:37.513827Z",
     "shell.execute_reply": "2024-09-21T18:13:37.512548Z",
     "shell.execute_reply.started": "2024-09-21T18:13:37.508775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_ture_list = y_true.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T18:14:03.639324Z",
     "iopub.status.busy": "2024-09-21T18:14:03.638899Z",
     "iopub.status.idle": "2024-09-21T18:14:03.657961Z",
     "shell.execute_reply": "2024-09-21T18:14:03.657005Z",
     "shell.execute_reply.started": "2024-09-21T18:14:03.639279Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model Evaluation:\n",
      "Accuracy: 0.926\n",
      "Accuracy for option ক: 0.842\n",
      "Accuracy for option খ: 1.000\n",
      "Accuracy for option গ: 0.923\n",
      "Accuracy for option ঘ: 0.947\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           ক       0.97      0.84      0.90        38\n",
      "           খ       0.79      1.00      0.88        34\n",
      "           গ       0.97      0.92      0.95        39\n",
      "           ঘ       1.00      0.95      0.97        38\n",
      "\n",
      "    accuracy                           0.93       149\n",
      "   macro avg       0.93      0.93      0.93       149\n",
      "weighted avg       0.94      0.93      0.93       149\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32  5  1  0]\n",
      " [ 0 34  0  0]\n",
      " [ 0  3 36  0]\n",
      " [ 1  1  0 36]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model\n",
    "print(\"Fine-tuned Model Evaluation:\")\n",
    "evaluate(y_ture_list, y_pred_fine_tuned)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5378595,
     "sourceId": 8939193,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 81881,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
